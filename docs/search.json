[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/mne abstract/index.html",
    "href": "posts/mne abstract/index.html",
    "title": "MNE Abstract",
    "section": "",
    "text": "Subanesthetic ketamine is a rapid and effective treatment for depression in individuals who have not responded to other treatments. Several lines of evidence demonstrate initial effects of ketamine in anterior cingulate/medial prefrontal cortex. Nonetheless, well-established, harmful effects of ketamine include dissociation, psychosis, and dependency with increasing dose and chronicity.\n\n\n\nIn this reproducible and open analysis pipeline the effects of sub-anesthetic doses of ketamine on auditory SG and spontaneous gamma in rats were investigated. Data for this computational study were previously collected at another institution where Sprague Dawley rats were injected (i.m.) in a multi-baseline (saline), counterbalanced alternating-treatments design of haloperidol (1 mg/kg), ketamine, and ketamine co-administered with haloperidol. The subanesthetic ketamine was administered between subjects in cohorts of 20 mg/kg, n=4 or 60 mg/kg, n=4. After initial surgery to implant microwire arrays bilaterally in medial prefrontal cortex, local field potentials were recorded in freely behaving animals. As computational reproducibility intermediate data and all code from this analysis pipeline is available including, pre-processing, data visualizations, and inferential statistics.\n\n\n\nKetamine reduced SG and increased the amplitude of baseline gamma-band activity in medial prefrontal cortex (mPFC) for both ketamine doses. Co-administration of the antipsychotic drug, haloperidol, partly reversed the reduction of SG in mPFC and partially reduced the observed increase of spontaneous beta and gamma band activity. Additionally, an increase of beta-band spontaneous activity occurred for the higher dose of ketamine (60 mg/kg), but not the lower dose. Co-administered ketamine and haloperidol blocked the increase of beta-band power for the high dose.\n\n\n\nThese findings suggest that the observed increase in gamma power at each site was potentially mediated by a local circuit mechanism, and that evoked potentials related to SG in mPFC and spontaneous intrinsic beta rhythm might be independently modulated from gamma rhythm. Consistent with these multi-neurotransmitter and receptor findings, the rapid antidepressant action of ketamine appears to arise from effects secondary and subsequent to the NMDA antagonist site of action."
  },
  {
    "objectID": "posts/mne abstract/index.html#ketamine-and-haloperidol-differentially-influence-auditory-inhibitory-gating-beta-and-gamma-activity-in-rat-medial-prefrontal-cortex",
    "href": "posts/mne abstract/index.html#ketamine-and-haloperidol-differentially-influence-auditory-inhibitory-gating-beta-and-gamma-activity-in-rat-medial-prefrontal-cortex",
    "title": "MNE Abstract",
    "section": "",
    "text": "Subanesthetic ketamine is a rapid and effective treatment for depression in individuals who have not responded to other treatments. Several lines of evidence demonstrate initial effects of ketamine in anterior cingulate/medial prefrontal cortex. Nonetheless, well-established, harmful effects of ketamine include dissociation, psychosis, and dependency with increasing dose and chronicity.\n\n\n\nIn this reproducible and open analysis pipeline the effects of sub-anesthetic doses of ketamine on auditory SG and spontaneous gamma in rats were investigated. Data for this computational study were previously collected at another institution where Sprague Dawley rats were injected (i.m.) in a multi-baseline (saline), counterbalanced alternating-treatments design of haloperidol (1 mg/kg), ketamine, and ketamine co-administered with haloperidol. The subanesthetic ketamine was administered between subjects in cohorts of 20 mg/kg, n=4 or 60 mg/kg, n=4. After initial surgery to implant microwire arrays bilaterally in medial prefrontal cortex, local field potentials were recorded in freely behaving animals. As computational reproducibility intermediate data and all code from this analysis pipeline is available including, pre-processing, data visualizations, and inferential statistics.\n\n\n\nKetamine reduced SG and increased the amplitude of baseline gamma-band activity in medial prefrontal cortex (mPFC) for both ketamine doses. Co-administration of the antipsychotic drug, haloperidol, partly reversed the reduction of SG in mPFC and partially reduced the observed increase of spontaneous beta and gamma band activity. Additionally, an increase of beta-band spontaneous activity occurred for the higher dose of ketamine (60 mg/kg), but not the lower dose. Co-administered ketamine and haloperidol blocked the increase of beta-band power for the high dose.\n\n\n\nThese findings suggest that the observed increase in gamma power at each site was potentially mediated by a local circuit mechanism, and that evoked potentials related to SG in mPFC and spontaneous intrinsic beta rhythm might be independently modulated from gamma rhythm. Consistent with these multi-neurotransmitter and receptor findings, the rapid antidepressant action of ketamine appears to arise from effects secondary and subsequent to the NMDA antagonist site of action."
  },
  {
    "objectID": "posts/test_post/index.html",
    "href": "posts/test_post/index.html",
    "title": "Post template",
    "section": "",
    "text": "This is a test post."
  },
  {
    "objectID": "posts/problem_solving_in_20_questions/index.html",
    "href": "posts/problem_solving_in_20_questions/index.html",
    "title": "Problem Solving Code in 20 Questions",
    "section": "",
    "text": "AI helped me to solve an audio segmentation problem that has dogged me for the past year.\nThe problem was some videotapes where sound stimuli are clearly audible, but continuous random ambient noise interferes with any attempt to precisely isolate the stimuls onsets. I need stimulus event onset timestamps isolated around 16-33 millisecond precision - complicated by the fact that digital video conversion has numerous randomly dropped frames. (Thus, I cannot rely on elapsed time as a useful metric.) The past 7 months I have much code only to find reason after reason why this is a difficult problem to solve.\nThen, I found out how to ask how to solve the problem by telling chatGPT to ask me questions in a particular context until it has enough information to write a method to interface with an API of any program (but preferably python). The resultant method is simple. It does exactly what I needed it to do using a python library that I was previously aware of, but I really didn‚Äôt know how to use the library.\nThis really is a game changer when it comes to teaching novices to code. I cannot make anyone want to learn to code, but the frustration level will certainly be different, because AI is really good with explaining concisely and/or overexplaining simple things. The difference from doing a Google search for answers is that you sort of need to know an answer in advance in order to come up with the right filtering parameters to eliminate all the junk information. Anyone in academia who sees AI as a threat to education has never really tried it."
  },
  {
    "objectID": "posts/use_rsync_on_hpg/index.html",
    "href": "posts/use_rsync_on_hpg/index.html",
    "title": "Post template",
    "section": "",
    "text": "A better way is to copy all the files from the ‚Äúpsy4911/share‚Äù folder to your HiPerGator (HPG) folder. The rsync command for moving files between directories on HPG is described here. Copying_files_between_directories with rsync You will want to go to the shared folder on HPG first and open a terminal (see button outlined green in the schreenshot below). \nThen if you copy and paste these commands into terminal it will do the work for you: rsync -av /blue/psy4911/share/ket_ephys/ket_annot /blue/psy4911/yourusername/ket_ephys/ rsync -av /blue/psy4911/share/ket_ephys/ket_annot_segs /blue/psy4911/yourusername/ket_ephys/ rsync -av /blue/psy4911/share/ket_ephys/hi-ket /blue/psy4911/yourusername/ket_ephys/\nFor the files in the hi-ket folder there‚Äôs another script (and a package named NEO ) that‚Äôs required to convert them from .plx files to .fif files.\nRequired python packages: - NEO - NEO repo - NEO documentation - MNE - MNE Tutorial: importing data from NEO\nCode for the file conversion:\nThe main thing is that it takes 15 minutes per file on hipergator (HPG) when it‚Äôs set up like a laptop computer. When you configure HPG to run your jupter notebook in high-performance mode, it takes only a minute or two per file. So, what would take 5 hours for 20 files, ends up taking only a half hour.\nAn even bigger savings comes when doing machine learning / AI with deeplabcut on HPG."
  },
  {
    "objectID": "posts/6p-RTour/index.html",
    "href": "posts/6p-RTour/index.html",
    "title": "R-tour in 6 pages",
    "section": "",
    "text": "RCSTour6p Slides"
  },
  {
    "objectID": "posts/hpg_setup_mpfc_ket/index.html",
    "href": "posts/hpg_setup_mpfc_ket/index.html",
    "title": "Setup HiPerGator for DeepLabCut",
    "section": "",
    "text": "UF Research Computing Help & Documentation (link)"
  },
  {
    "objectID": "posts/hpg_setup_mpfc_ket/index.html#navigate-to-hipergator-through-openondemand",
    "href": "posts/hpg_setup_mpfc_ket/index.html#navigate-to-hipergator-through-openondemand",
    "title": "Setup HiPerGator for DeepLabCut",
    "section": "Navigate to HiPerGator through OpenOnDemand",
    "text": "Navigate to HiPerGator through OpenOnDemand\n\nTo start off, type in your browser: https://ood.rc.ufl.edu\nIf this is the first time going to OpenOnDemand in your browser there will be a page re-direct to InCommon.org\n\nFrom this re-direct website type Florida in the institution and select ‚ÄúUniversity of Florida‚Äù from the dropdown menu.\nFrom here you would login with 2-factor authentication as you typically would for Canvas.\n\nAfter OpenOnDemand successfully loads select the Files dropdown\n\nSelect /blue/psy4911\n\nNext, when you are in the correct location you can open a terminal to create a shortcut so that you can always find your blue-folder\n\nType a symlink command in the terminal: ln -s /blue/psy4911 blue_psy4911\n\nConfirm that the shortcut was made by typing: find . -maxdepth l type l -ls"
  },
  {
    "objectID": "posts/hpg_setup_mpfc_ket/index.html#create-a-folder-for-downloading-files-and-examples-to-your-blue-drive-for-learning-deeplabcut",
    "href": "posts/hpg_setup_mpfc_ket/index.html#create-a-folder-for-downloading-files-and-examples-to-your-blue-drive-for-learning-deeplabcut",
    "title": "Setup HiPerGator for DeepLabCut",
    "section": "Create a folder for downloading files and examples to your blue drive for learning DeepLabCut",
    "text": "Create a folder for downloading files and examples to your blue drive for learning DeepLabCut\n\nThere are two ways to navigate the blue drive filesystem: OpenOnDemand GUI & Terminal (command line)\n\nif you‚Äôre not sure how to navigate to a directory from the command line go to these links:\n\nIntro to command line\nCommand line tips\nUnderstanding file paths from the command line\n\n\nCreate a new folder so that you can do DeepLabCut tutorials and examples\n\n\nNavigate to your blue drive personal folder ‚Äú/blue/psy4911/yourname/‚Äù then make a folder named try-dlc\n\n\nThe easiest way to start out in terminal from OOD is to click on the folder that you made\n\n‚Ä¶then from that location click Open in Terminal"
  },
  {
    "objectID": "posts/hpg_setup_mpfc_ket/index.html#clone-all-example-material-from-github-to-a-folder-on-your-blue-drive",
    "href": "posts/hpg_setup_mpfc_ket/index.html#clone-all-example-material-from-github-to-a-folder-on-your-blue-drive",
    "title": "Setup HiPerGator for DeepLabCut",
    "section": "Clone all example material from GitHub to a folder on your Blue Drive",
    "text": "Clone all example material from GitHub to a folder on your Blue Drive\n\nFrom the chosen folder on your /blue/psy4911/yourname/ folder install DeepLabCut using the following commands in terminal:\n\nmodule load conda\ngit clone https://github.com/DeepLabCut/DeepLabCut.git\n\nNavigate to the new folder /blue/psy4911/yourname/DeepLabCut/ by typing cd DeepLabCut/deeplabcut\nto run from the command line type module load deeplabcut/2.2.1\nthen type ipython\nif you type ?deeplabcut on the command line you can see that everything in DeepLabCut 2.0 is available from the command line\ntype exit() to leave ipython\n\n\n\n\nNEXT go to the HiPerGator DeepLabCut Jupyter Notebooks tutorial‚Ä¶\n\nTo Run Napari from the remote GUI on HPG start HiperGator Desktop then open a terminal on the desktop and type:\n\nmodule spider Napari\nmodule load napari/0.4.7\nNapari"
  },
  {
    "objectID": "posts/make-gh-blogpost/index.html",
    "href": "posts/make-gh-blogpost/index.html",
    "title": "How to make a wiki document.",
    "section": "",
    "text": "This is how to start a post."
  },
  {
    "objectID": "posts/cwppwc/index.html",
    "href": "posts/cwppwc/index.html",
    "title": "Posting Wiki Content",
    "section": "",
    "text": "Markdown is a markup coding language that can be used to convert content written in a plain-text format into HTML. It can be used on Github to make easy to read webpages: like the one you‚Äôre reading now!\n\n\nThe first thing you should do when creating any article is filling out the most basic information:\n\n\n\n```{yaml, eval=false}\n\n\n\n\ntitle: ‚ÄúWrite the title of your article in quotes‚Äù\n\n\nauthor: your name goes here!\n\n\ndescription: what is your article about? briefly summarize it\n\n\ndate: write the date that you publish your article\n\n\n\n```\nAs shown above, you are always going to put these four basic pieces of information between a pair of three dashes. While the title, author, description, and date are always going to be in the section, you also have the option to add categories to this section: categories: [put your category names in a pair of brackets, seperate each category with a comma] Categories allow us to organize the articles on this wiki by topic. When you open the wiki, you will see a list of categories on the right-hand side. When you click on a category, you will be able to see any of the articles tagged with that specific category in the manner depicted above.\n\n\n\nIn addition to setting up important information about your article, markdown can be used to format your main body of text in a number of ways‚Ä¶\nYou can create a header by using a pound sign: # Heading\nYou can make a subheading, a sub-subheading, and onwards just by adding more pound signs: ## Subheading ### Sub-subheading\nYou can italicize or bold a sentence using asterisks: *This sentence is italicized* **This sentence is bolded**\nLinks and images can be inbedded like so: !(text)[link]\nIts also possible to inbed code directly into the text - but we‚Äôll be going over that later."
  },
  {
    "objectID": "posts/cwppwc/index.html#converting-markdown-syntax",
    "href": "posts/cwppwc/index.html#converting-markdown-syntax",
    "title": "Posting Wiki Content",
    "section": "",
    "text": "Markdown is a markup coding language that can be used to convert content written in a plain-text format into HTML. It can be used on Github to make easy to read webpages: like the one you‚Äôre reading now!\n\n\nThe first thing you should do when creating any article is filling out the most basic information:\n\n\n\n```{yaml, eval=false}\n\n\n\n\ntitle: ‚ÄúWrite the title of your article in quotes‚Äù\n\n\nauthor: your name goes here!\n\n\ndescription: what is your article about? briefly summarize it\n\n\ndate: write the date that you publish your article\n\n\n\n```\nAs shown above, you are always going to put these four basic pieces of information between a pair of three dashes. While the title, author, description, and date are always going to be in the section, you also have the option to add categories to this section: categories: [put your category names in a pair of brackets, seperate each category with a comma] Categories allow us to organize the articles on this wiki by topic. When you open the wiki, you will see a list of categories on the right-hand side. When you click on a category, you will be able to see any of the articles tagged with that specific category in the manner depicted above.\n\n\n\nIn addition to setting up important information about your article, markdown can be used to format your main body of text in a number of ways‚Ä¶\nYou can create a header by using a pound sign: # Heading\nYou can make a subheading, a sub-subheading, and onwards just by adding more pound signs: ## Subheading ### Sub-subheading\nYou can italicize or bold a sentence using asterisks: *This sentence is italicized* **This sentence is bolded**\nLinks and images can be inbedded like so: !(text)[link]\nIts also possible to inbed code directly into the text - but we‚Äôll be going over that later."
  },
  {
    "objectID": "posts/cwppwc/index.html#importance-of-yaml",
    "href": "posts/cwppwc/index.html#importance-of-yaml",
    "title": "Posting Wiki Content",
    "section": "Importance of YAML",
    "text": "Importance of YAML"
  },
  {
    "objectID": "posts/cwppwc/index.html#quarto",
    "href": "posts/cwppwc/index.html#quarto",
    "title": "Posting Wiki Content",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "posts/cwppwc/index.html#creating-a-folder",
    "href": "posts/cwppwc/index.html#creating-a-folder",
    "title": "Posting Wiki Content",
    "section": "Creating a Folder",
    "text": "Creating a Folder\nIn order to create an article, you first have to create a folder in which you will create an index.qmd file. The index.qmd file is where you will be writing in Markdown to create your article. The process of folder creation is depicted in the gif below."
  },
  {
    "objectID": "posts/cwppwc/index.html#embedding-code",
    "href": "posts/cwppwc/index.html#embedding-code",
    "title": "Posting Wiki Content",
    "section": "Embedding Code",
    "text": "Embedding Code"
  },
  {
    "objectID": "posts/why-code/index.html",
    "href": "posts/why-code/index.html",
    "title": "Why Code?",
    "section": "",
    "text": "This talk introduced advantages and potential obstacles to using script-based pipelines. We discussed how capabilities of scientific python ecosystem are leveraged and organized in MNE in order to compose reproducible analysis pipelines that are easy to understand and efficient to use. The extensive documentation of the MNE website provides instructions for installation and configuration. Extensive examples and tutorials on the MNE documentation website allows users to benefit directly by searching, downloading, testing and incrementally adapting code.\nIntroduction to Scripting with MNE-Python"
  },
  {
    "objectID": "posts/csea-matlab-tutorial-links/index.html",
    "href": "posts/csea-matlab-tutorial-links/index.html",
    "title": "CSEA MatLab tutorials",
    "section": "",
    "text": "This is a post with executable code.\nMatlab Livescripts examples Folder\nThe following examples are in the GitHub folder:\n\nCrossfreq_coupling.mlx\nFourier_demo.mlx\nGrangerCausality.mlx\nHilbertPhaseDemo.mlx\nIRF_findsingletrialbetas.mlx\nMNE_SourceEstimation.mlx\nMicroSaCondiSpas.mlx\nde_convolution.mlx\nfilters.mlx\nwavelet_walkthru.mlx"
  },
  {
    "objectID": "posts/MNE-Python-EEG/index.html",
    "href": "posts/MNE-Python-EEG/index.html",
    "title": "MNE-Python: EEG",
    "section": "",
    "text": "MNE-Python: EEG Analysis"
  },
  {
    "objectID": "posts/use_ssh_hpg3/index.html",
    "href": "posts/use_ssh_hpg3/index.html",
    "title": "SSH to HPG3 and run MNE-BIDS-Pipeline",
    "section": "",
    "text": "Type the following command in the terminal:\nssh urgatoruser@hpg.rc.ufl.edu\nThen, follow the prompts to enter your password and Duo two-factor authentication.\n(base) % ssh urgatoruser@hpg.rc.ufl.edu\nPassword: xxxxxxxx\nDuo two-factor login for urgatoruser@ufl.edu\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-1809\n 2. Phone call to XXX-XXX-1809\n\nPasscode or option (1-2): 1\nWorks through the UF Single-sign on DUO app similar to for Canvas and gatormail.\n\n\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=2 --mem=2gb -t 90 --pty bash -i\n[mygatoruser@login1 ~]$ srun --pty -p hpg2-compute -n 1 -N 1 -t 90 --mem=2gb /bin/bash\n\n\n\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i\nsrun: job 4572441 queued and waiting for resources\nsrun: job 4572441 has been allocated resources\nNow you are on the compute node. You can check the hostname and the node list with the following commands:\n[mygatoruser@c0711a-s6 data]$ echo \"Hello from $(hostname)\"\nHello from c0711a-s6.ufhpc\n\necho $SLURM_JOB_NODELIST\nc0711a-s6"
  },
  {
    "objectID": "posts/use_ssh_hpg3/index.html#a-single-node-2-cpu-core-job-with-2gb-of-ram-for-90-minutes-can-be-started-with-the-following-command",
    "href": "posts/use_ssh_hpg3/index.html#a-single-node-2-cpu-core-job-with-2gb-of-ram-for-90-minutes-can-be-started-with-the-following-command",
    "title": "SSH to HPG3 and run MNE-BIDS-Pipeline",
    "section": "",
    "text": "srun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=2 --mem=2gb -t 90 --pty bash -i\n[mygatoruser@login1 ~]$ srun --pty -p hpg2-compute -n 1 -N 1 -t 90 --mem=2gb /bin/bash"
  },
  {
    "objectID": "posts/use_ssh_hpg3/index.html#a-single-node-4-cpu-core-job-with-28gb-of-ram-for-120-minutes-can-be-started-with-the-following-command",
    "href": "posts/use_ssh_hpg3/index.html#a-single-node-4-cpu-core-job-with-28gb-of-ram-for-120-minutes-can-be-started-with-the-following-command",
    "title": "SSH to HPG3 and run MNE-BIDS-Pipeline",
    "section": "",
    "text": "srun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i\nsrun: job 4572441 queued and waiting for resources\nsrun: job 4572441 has been allocated resources\nNow you are on the compute node. You can check the hostname and the node list with the following commands:\n[mygatoruser@c0711a-s6 data]$ echo \"Hello from $(hostname)\"\nHello from c0711a-s6.ufhpc\n\necho $SLURM_JOB_NODELIST\nc0711a-s6"
  },
  {
    "objectID": "posts/use_ssh_hpg3/index.html#random-tips",
    "href": "posts/use_ssh_hpg3/index.html#random-tips",
    "title": "SSH to HPG3 and run MNE-BIDS-Pipeline",
    "section": "Random Tips:",
    "text": "Random Tips:\nCreate a symbolic link to your group storage directory on Blue drive:\nln -s /blue/your_group/urgatoruser/ blue\nUsing OpenOn Demand for file management\nTroubleshooting HPG3 Open OnDemand\nUsing rsync to copy/move files on HPG3\nmacOS Development Environment: iTerm2, oh-my-zsh, and VS Code"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "",
    "text": "Convolution: a simple mathematical function that quantifies similarity between a pattern (a ‚Äúkernel‚Äù) such as the red square wave below with data ( blue rectangularish thingy below).\n\n\nThe convolution (black line) reflects how similar the blue signal is with the kernel at any given time point. Convolution is thus used in all of digital signal processing."
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-1",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-1",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "",
    "text": "The math is simple. We take the kernel, move it to the first portion of the data by lining it up with the first sample point, then we multiply each element of the kernel with the data, sum the results into one new number and write that to that first sample point.\n\n%matplotlib inline\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#convolution-in-signal-generation-and-in-fmri-research.",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#convolution-in-signal-generation-and-in-fmri-research.",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "Convolution in signal generation and in fMRI research.",
    "text": "Convolution in signal generation and in fMRI research.\nif we want to generate a signal where a certain pattern (the kernel) occurs at certain times, we can use convolution to achieve that. Thus, programs liek SPM use convolution of stimulus timinng info with a lernel that resembles the typical BOLD (fMRI) response as the kernel to generate fake, ideal, fMRI activation for comparison/correlation with the real data:\n\n\nonsets = np.zeros(120)\nonsets[1:11:1] = 1\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#make-a-kernel",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#make-a-kernel",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "Make a Kernel",
    "text": "Make a Kernel\n\nNow we make a kernel.\n\nthe inverted gamma function is nice because it looks like fMRI in V1\nlet‚Äôs make one that is 10 scans long\n\n\n\ng_f = np.arange(0.1,5.1,0.5)\nkernel = [1./math.gamma(i) for i in g_f]\nplt.plot(kernel) #, title('an inverted gamma kernel, length 10')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#convolve",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#convolve",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "convolve",
    "text": "convolve\nNow we just convolve the onsets with the kernel and plot the result\n\n\ng_f = np.arange(0.1,5,0.5)\nkernel = [1./math.gamma(i) for i in g_f]\nplt.plot(kernel) #, title('an inverted gamma kernel, length 10')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#longer-onsets",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#longer-onsets",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "longer onsets",
    "text": "longer onsets\nSo this basically puts each 10 second kernel at the location where the onset is. There were 11 seconds between onsets, so this is like copy paste, but how about when the kernel is longer than the interval between onsets?\n\n\nconvolution  = np.convolve(onsets, kernel); \n\nplt.plot(convolution) #, title('onset vector convolved with canonical BOLD response')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#but",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#but",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "but",
    "text": "but\nSo this basically puts each 10 second kernel at the location where the onset is. There were 11 seconds between onsets, so this is like copy paste, but how about when the kernel is longer than the interval between onsets?\n\n\ng_f=np.arange(0.1,5.1,0.25)\nkernel = [1./math.gamma(i) for i in g_f] # this Kernel is twice as long\nconvolution  = np.convolve(onsets, kernel); \n#figure\nplt.plot(convolution) #, title('Convolution with temporal summation')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#overlaps",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#overlaps",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "overlaps",
    "text": "overlaps\nWhen the convolution encounters overlap, then temporal summation results, because convolution is a process of shifting the kernel, multiplying element-wise, and summation to one new value, rinse and repeat. Because of the shifting and summing up portion of the algorithm, if the Kernel spans multiple, similar events, it will show temporal summation."
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#even-better",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#even-better",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "even better‚Ä¶",
    "text": "even better‚Ä¶\nThis is more interesting with variable inter-onset times.\n\n\nonsets = np.zeros((120,1))\n\nonsets[[3,14,22,36,46,50,66,86,91,106,115],] = 1; #simple case where a stimulus is on every 11 scans\nprint(onsets.shape)\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')\n\n\n(120, 1)"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#now-we-convolve-these-onset-times-with-the-same-kernel",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#now-we-convolve-these-onset-times-with-the-same-kernel",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "Now we convolve these onset times with the same kernel",
    "text": "Now we convolve these onset times with the same kernel\n\n\nconvolution  = np.convolve(onsets.squeeze(), kernel); \nplt.plot(convolution) #, title('Convolution of random ITIs')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#what-happened",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#what-happened",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "what happened?",
    "text": "what happened?\nLonger intervals between onsets prompt complete reduction to baseline, temporal proximity prompts smeared, overlapping events. How about gamma-shaped responses to stimuli that are longer than one image?\n\n\nonsets = np.zeros((120,1))\nset0=[3,14,22,36,46,50,66,86,91,106,115];\nset1=[i+1 for i in set0]\nset2=[i+2 for i in set0]\nset3=[i+3 for i in set0]\nonsets[set0,] = 1; #simple case where a stimulus is on every 11 scans\nonsets[set1,] = 1;\n\nonsets[set2,] = 1;#simple case where a stimulus is on every 11 scans\nonsets[set3,] = 1;\n##onsets[[3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]+2,] = 1;\n#onsets[[3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]+3,] = 1;\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#next",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#next",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "next",
    "text": "next\nNow we convolve these onset times with the same kernel\n\n\nconvolution  = np.convolve(onsets.squeeze(), kernel); \n\nplt.plot(convolution) #, title('Convolution with 4-TR-long events'), ylabel('note the scale')"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#deconvolution-1",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#deconvolution-1",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "Deconvolution",
    "text": "Deconvolution\ndeconvolution is the process where we wish to estimate the Kernel from known data and known event-times. This is a version of the so-called inverse problem , and we solve it with a regression. Let‚Äôs start with the simulated we we have:\n\n\nconvolution = np.random.rand(200) # Replace with your own convolution array\nonsets = np.random.rand(120) # Replace with your own onsets array\n\nX = np.zeros((len(convolution[0:120]), 20))\ntemp = onsets.copy()\n\nfor i in range(20):\n    X[:, i] = temp\n    temp = np.concatenate(([0], temp[:-1]))\n\nplt.pcolor(X, cmap='gray')\nplt.show()"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-2",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-2",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "",
    "text": "PX = np.linalg.pinv(X)\n\nplt.pcolor(PX, cmap='gray')\nplt.show()\n\nh = PX @ convolution[0:120].T"
  },
  {
    "objectID": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-3",
    "href": "posts/matlab2python_tutorials/jupyter_quarto_deconv/de_convolution.html#section-3",
    "title": "Convolution and deconvolution: an Illustration",
    "section": "",
    "text": "convolution = np.random.rand(200) # Replace with your own convolution array\nonsets = [3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]\n\n### WHAT IS hrf_est()? ###\n#h = hrf_est(convolution[0:120], onsets, 20)\n\n# COULD IT BE?\n\n# def hrf_est(convolution, onsets, n):\n#     X = np.zeros(((convolution).,n))\n#     temp = onsets.copy()\n# \n#     for i in range(n):\n#         X[:,i] = temp\n#         temp = np.concatenate(([0], temp[:-1]))\n# \n#     PX = np.linalg.pinv(X)\n#     h = PX @ convolution[0:120].T\n#     return h\n\n#plt.plot(h)\n#plt.show()"
  },
  {
    "objectID": "posts/main_mne_bids_pipeline/index.html",
    "href": "posts/main_mne_bids_pipeline/index.html",
    "title": "Understanding How MNE-BIDS_pipeline builds",
    "section": "",
    "text": "The provided Python script is a command-line utility for a pipeline, typically used for processing data. Here‚Äôs a breakdown of its functionality and structure:\n\nImports and Definitions: The script starts by importing necessary modules and defining some functions. This includes modules for argument parsing (argparse), file path handling (pathlib), logging, parallel processing, and specific functions from other modules within the same package (prefixed with _).\nArgument Parsing: The script uses the argparse module to define command-line arguments that control the behavior of the pipeline. These arguments include:\n\nconfig: Path to a configuration file specifying pipeline settings.\ncreate-config: Creates a template configuration file.\nsteps: Defines specific processing steps or groups of steps to run.\nroot-dir, deriv_root, subject, session, task, run: Specify paths and identifiers for the data to process.\nn_jobs: Number of parallel processes to execute.\ninteractive, debug, no-cache: Flags for interactive mode, debugging, and disabling caching.\n\nArgument Validation: The script checks if the provided arguments are valid, particularly focusing on the configuration file.\nProcessing Steps Identification: Based on the steps argument, the script identifies which processing steps or stages are to be executed. It does this by parsing the steps argument and mapping them to corresponding modules.\nConfiguration Loading and Overrides: The script loads the pipeline configuration from the specified file and applies any overrides from the command-line arguments.\nPipeline Execution:\n\nThe script iterates over the identified processing steps.\nFor each step, it logs the start, executes the main function of the corresponding module (which is where the actual processing happens), and then logs the time taken for the step.\nThis execution can be parallelized based on the n_jobs argument.\n\nLogging and Error Handling: Throughout its execution, the script logs various messages, including errors and execution time for each step. The --debug option enables additional debugging information on error.\nPipeline Configuration and Modular Design: The script is designed to be modular, where each step in the pipeline is encapsulated in a separate module. The pipeline‚Äôs behavior is controlled by a configuration file, allowing for flexible and customizable data processing.\nInteractive and Cache Options: The script supports interactive mode and can disable caching of intermediate results, providing flexibility for different use cases.\n\nOverall, the script is a command-line interface for a data processing pipeline, where the specific processing steps are modularized and controlled via a configuration file. The use of command-line arguments allows for flexible execution of different parts of the pipeline.\n\nimport argparse\nimport pathlib\nfrom textwrap import dedent\nimport time\nfrom typing import List\nfrom types import ModuleType, SimpleNamespace\n\nimport numpy as np\n\n\nfrom ._config_utils import _get_step_modules\nfrom ._config_import import _import_config\nfrom ._config_template import create_template_config\nfrom ._logging import logger, gen_log_kwargs\nfrom ._parallel import get_parallel_backend\nfrom ._run import _short_step_path\n\n\ndef main():\n    from . import __version__\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n    parser.add_argument(\"config\", nargs=\"?\", default=None)\n    parser.add_argument(\n        \"--config\",\n        dest=\"config_switch\",\n        default=None,\n        metavar=\"FILE\",\n        help=\"The path of the pipeline configuration file to use.\",\n    )\n    parser.add_argument(\n        \"--create-config\",\n        dest=\"create_config\",\n        default=None,\n        metavar=\"FILE\",\n        help=\"Create a template configuration file with the specified name. \"\n        \"If specified, all other parameters will be ignored.\",\n    ),\n    parser.add_argument(\n        \"--steps\",\n        dest=\"steps\",\n        default=\"all\",\n        help=dedent(\n            \"\"\"\\\n        The processing steps to run.\n        Can either be one of the processing groups 'preprocessing', sensor',\n        'source', 'report',  or 'all',  or the name of a processing group plus\n        the desired step sans the step number and\n        filename extension, separated by a '/'. For example, to run ICA, you\n        would pass 'sensor/run_ica`. If unspecified, will run all processing\n        steps. Can also be a tuple of steps.\"\"\"\n        ),\n    )\n    parser.add_argument(\n        \"--root-dir\",\n        dest=\"root_dir\",\n        default=None,\n        help=\"BIDS root directory of the data to process.\",\n    )\n    parser.add_argument(\n        \"--deriv_root\",\n        dest=\"deriv_root\",\n        default=None,\n        help=dedent(\n            \"\"\"\\\n        The root of the derivatives directory\n        in which the pipeline will store the processing results.\n        If unspecified, this will be derivatives/mne-bids-pipeline\n        inside the BIDS root.\"\"\"\n        ),\n    ),\n    parser.add_argument(\n        \"--subject\", dest=\"subject\", default=None, help=\"The subject to process.\"\n    )\n    parser.add_argument(\n        \"--session\", dest=\"session\", default=None, help=\"The session to process.\"\n    )\n    parser.add_argument(\n        \"--task\", dest=\"task\", default=None, help=\"The task to process.\"\n    )\n    parser.add_argument(\"--run\", dest=\"run\", default=None, help=\"The run to process.\")\n    parser.add_argument(\n        \"--n_jobs\",\n        dest=\"n_jobs\",\n        type=int,\n        default=None,\n        help=\"The number of parallel processes to execute.\",\n    )\n    parser.add_argument(\n        \"--interactive\",\n        dest=\"interactive\",\n        action=\"store_true\",\n        help=\"Enable interactive mode.\",\n    )\n    parser.add_argument(\n        \"--debug\", dest=\"debug\", action=\"store_true\", help=\"Enable debugging on error.\"\n    )\n    parser.add_argument(\n        \"--no-cache\",\n        dest=\"no_cache\",\n        action=\"store_true\",\n        help=\"Disable caching of intermediate results.\",\n    )\n    options = parser.parse_args()\n\n    if options.create_config is not None:\n        target_path = pathlib.Path(options.create_config)\n        create_template_config(target_path=target_path, overwrite=False)\n        return\n\n    config = options.config\n    config_switch = options.config_switch\n    bad = False\n    if config is None:\n        if config_switch is None:\n            bad = \"neither was provided\"\n        else:\n            config = config_switch\n    elif config_switch is not None:\n        bad = \"both were provided\"\n    if bad:\n        parser.error(\n            \"‚ùå You must specify a configuration file either as a single \"\n            f\"argument or with --config, but {bad}.\"\n        )\n    steps = options.steps\n    root_dir = options.root_dir\n    deriv_root = options.deriv_root\n    subject, session = options.subject, options.session\n    task, run = options.task, options.run\n    n_jobs = options.n_jobs\n    interactive, debug = options.interactive, options.debug\n    cache = not options.no_cache\n\n    if isinstance(steps, str) and \",\" in steps:\n        # Work around limitation in Fire: --steps=foo,bar/baz won't produce a\n        # tuple ('foo', 'bar/baz'), but a string 'foo,bar/baz'.\n        steps = tuple(steps.split(\",\"))\n    elif isinstance(steps, str):\n        steps = (steps,)\n\n    on_error = \"debug\" if debug else None\n    cache = \"1\" if cache else \"0\"\n\n    processing_stages = []\n    processing_steps = []\n    for steps_ in steps:\n        if \"/\" in steps_:\n            stage, step = steps_.split(\"/\")\n            processing_stages.append(stage)\n            processing_steps.append(step)\n        else:\n            # User specified \"sensor\", \"preprocessing\" or similar, but without\n            # any further grouping.\n            processing_stages.append(steps_)\n            processing_steps.append(None)\n\n    config_path = pathlib.Path(config).expanduser().resolve(strict=True)\n    overrides = SimpleNamespace()\n    if root_dir:\n        overrides.bids_root = pathlib.Path(root_dir).expanduser().resolve(strict=True)\n    if deriv_root:\n        overrides.deriv_root = (\n            pathlib.Path(deriv_root).expanduser().resolve(strict=False)\n        )\n    if subject:\n        overrides.subjects = [subject]\n    if session:\n        overrides.sessions = [session]\n    if task:\n        overrides.task = task\n    if run:\n        overrides.runs = run\n    if interactive:\n        overrides.interactive = interactive\n    if n_jobs:\n        overrides.n_jobs = int(n_jobs)\n    if on_error:\n        overrides.on_error = on_error\n    if not cache:\n        overrides.memory_location = False\n\n    step_modules: List[ModuleType] = []\n    STEP_MODULES = _get_step_modules()\n    for stage, step in zip(processing_stages, processing_steps):\n        if stage not in STEP_MODULES.keys():\n            raise ValueError(\n                f\"Invalid step requested: '{stage}'. \"\n                f\"It should be one of {list(STEP_MODULES.keys())}.\"\n            )\n\n        if step is None:\n            # User specified `sensors`, `source`, or similar\n            step_modules.extend(STEP_MODULES[stage])\n        else:\n            # User specified 'stage/step'\n            for step_module in STEP_MODULES[stage]:\n                step_name = pathlib.Path(step_module.__file__).name\n                if step in step_name:\n                    step_modules.append(step_module)\n                    break\n            else:\n                # We've iterated over all steps, but none matched!\n                raise ValueError(f\"Invalid steps requested: {stage}/{step}\")\n\n    if processing_stages[0] != \"all\":\n        # Always run the directory initialization steps, but skip for 'all',\n        # because it already includes them ‚Äì and we want to avoid running\n        # them twice.\n        step_modules = [*STEP_MODULES[\"init\"], *step_modules]\n\n    logger.title(\"Welcome aboard MNE-BIDS-Pipeline! üëã\")\n    msg = f\"Using configuration: {config}\"\n    __mne_bids_pipeline_step__ = pathlib.Path(__file__)  # used for logging\n    logger.info(**gen_log_kwargs(message=msg, emoji=\"üìù\"))\n    config_imported = _import_config(\n        config_path=config_path,\n        overrides=overrides,\n    )\n    # Initialize dask now\n    with get_parallel_backend(config_imported.exec_params):\n        pass\n    del __mne_bids_pipeline_step__\n    logger.end()\n\n    for step_module in step_modules:\n        start = time.time()\n        step = _short_step_path(pathlib.Path(step_module.__file__))\n        logger.title(title=f\"{step}\")\n        step_module.main(config=config_imported)\n        elapsed = time.time() - start\n        hours, remainder = divmod(elapsed, 3600)\n        hours = int(hours)\n        minutes, seconds = divmod(remainder, 60)\n        minutes = int(minutes)\n        seconds = int(np.ceil(seconds))  # always take full seconds\n        elapsed = f\"{seconds}s\"\n        if minutes:\n            elapsed = f\"{minutes}m {elapsed}\"\n        if hours:\n            elapsed = f\"{hours}h {elapsed}\"\n        logger.end(f\"done ({elapsed})\")"
  },
  {
    "objectID": "posts/open-sci-github-intro/index.html",
    "href": "posts/open-sci-github-intro/index.html",
    "title": "Introduction to GitHub for Open Science",
    "section": "",
    "text": "Open Science With GitHub: An Introduction"
  },
  {
    "objectID": "posts/open-sci-github-intro/index.html#overview",
    "href": "posts/open-sci-github-intro/index.html#overview",
    "title": "Introduction to GitHub for Open Science",
    "section": "Overview",
    "text": "Overview\nMuch of open science requires some technical knowledge that many novices find to be initially burdensome and confusing. Open source analysis tools are usually a work in progress. Documentation for these tools, if present at all, will be incomplete or out of date. The purpose of this slideshow is to explore concepts that will enable team coordination on software usage and code development projects. The hope is that users will be able to find the information that they need from development notes and communication between code authors. In the long-term, notes and comments about the intended function, rationale, and usage of code will be useful for outsiders but also for the project organizers and code developers themselves."
  },
  {
    "objectID": "posts/share-hpg-envs/index.html",
    "href": "posts/share-hpg-envs/index.html",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "",
    "text": "To simply use the environment go to\n\nGetting Started: ~/.condarc configuration file\nthen to Copy the template_kernelfolder to your path\n\nFor additional info about using Python virtual environments with Conda please go the the UFRC page or the Software Carpentries pages from which these procedures were derived.\n\nUFRC: Customizing a Shared Python Environment\nCarpentries: Ch.2 Conda Environments\nConda: Using the .condarc Configuration file"
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#custom-python-environments-for-jupyter-notebooks",
    "href": "posts/share-hpg-envs/index.html#custom-python-environments-for-jupyter-notebooks",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "",
    "text": "To simply use the environment go to\n\nGetting Started: ~/.condarc configuration file\nthen to Copy the template_kernelfolder to your path\n\nFor additional info about using Python virtual environments with Conda please go the the UFRC page or the Software Carpentries pages from which these procedures were derived.\n\nUFRC: Customizing a Shared Python Environment\nCarpentries: Ch.2 Conda Environments\nConda: Using the .condarc Configuration file"
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#background-from-ufrc-page",
    "href": "posts/share-hpg-envs/index.html#background-from-ufrc-page",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Background (from UFRC page)",
    "text": "Background (from UFRC page)\nMany projects that use Python code require careful management of the respective Python environments. Rapid changes in package dependencies, package version conflicts, deprecation of APIs (function calls) by individual projects, and obsolescence of system drivers and libraries make it virtually impossible to use an arbitrary set of packages or create one all-encompassing environment that will serve everyone‚Äôs needs over long periods of time. The high velocity of changes in the popular ML/DL frameworks and packages and GPU computing exacerbates the problem."
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#getting-started-conda-configuration",
    "href": "posts/share-hpg-envs/index.html#getting-started-conda-configuration",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Getting Started: Conda Configuration",
    "text": "Getting Started: Conda Configuration\n\nThe ~/.condarc configuration file\nconda‚Äòs behavior is controlled by a configuration file in your home directory called .condarc. The dot at the start of the name means that the file is hidden from ‚Äôls‚Äô file listing command by default. If you have not run conda before, you won‚Äôt have this file. Whether the file exists or not, the steps here will help you modify the file to work best on HiPerGator. First load of the conda environment module on HiPerGator will put the current ‚Äò‚Äôbest practice‚Äô‚Äô .condarc into your home directory.\nThis path will be found in your home directory (i.e., Home/&lt;myname&gt; is usually symbolized as : ~).\nHome/&lt;myname&gt;/.condarc\n\n\nconda package cache location\nconda caches (keeps a copy) of all downloaded packages by default in the ~/.conda/pkgs directory tree. If you install a lot of packages you may end up filling up your home quota. You can change the default package cache path. To do so, add or change the pkgs_dirs setting in your ~/.condarc configuration file e.g.:\n\n\npkgs_dirs:\n  - /blue/akeil/share/conda/pkgs\n\n‚Ä¶\nor\n\n  - /blue/akeil/$USER/conda/pkgs\n\n\nReplace akeil or mygroup with your actual group name.\n\n\nconda environment location\nconda puts all packages installed in a particular environment into a single directory. By default ‚Äò‚Äônamed‚Äô‚Äô conda environments are created in the ~/.conda/envs directory tree. They can quickly grow in size and, especially if you have many environments, fill the 40GB home directory quota. For example, the environment we will create in this training is 5.3GB in size. As such, it is important to use ‚Äò‚Äôpath‚Äô‚Äô based (conda create -p PATH) conda environments, which allow you to use any path for a particular environment for example allowing you to keep a project-specific conda environment close to the project data in /blue/ where you group has terrabyte(s) of space.\nYou can also change the default path for the ‚Äò‚Äôname‚Äô‚Äô environments (conda create -n NAME) if you prefer to keep all conda environments in the same directory tree. To do so, add or change the envs_dirs setting in the ~/.condarc configuration file e.g.:\n\n\nenvs_dirs:\n  - /blue/akeil/share/conda/envs\n\n‚Ä¶\nor\n\n- /blue/akeil/$USER/conda/envs\n\n\nReplace mygroup with your actual group name.\n\n\nEditing your ~/.condarc file.\nOne way to edit your ~/.condarc file is to type:\nnano ~/.condarc\nIf the file is empty, paste in the text below, editing the env_dirs: and pkg_dirs: as below. If the file has contents, update those lines.\n\n\n\n\n\n\nNote\n\n\n\nYour ~/.condarc should look something like this when you are done editing (again, replacing group-akeil and USER in the paths with your actual group and username).\n\n\n\nchannels: \n  - conda-forge \n  - bioconda \n  - defaults \nenvs_dirs: \n  - /blue/akeil/USER/conda/envs \npkgs_dirs: \n  - /blue/akeil/USER/conda/pkgs \nauto_activate_base: false \nauto_update_conda: false \nalways_yes: false \nshow_channel_urls: false"
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#use-your-kernel-from-command-line-or-scripts",
    "href": "posts/share-hpg-envs/index.html#use-your-kernel-from-command-line-or-scripts",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Use your kernel from command line or scripts",
    "text": "Use your kernel from command line or scripts\nNow that we have our environment ready, we can use it from the command line or a script using something like:\n\n\nmodule load conda\nconda activate mne\n\n# Run my python script\npython amazing_script.py\n\n‚Ä¶\nor\na path based setting:\n\n# Set path to environment \n#   pre-pend to PATH variable\nenv_path=/blue/akeil/share/mne_1_x/conda/bin\nexport PATH=$env_path:$PATH\n \n# Run my python script\npython amazing_script.py"
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#setup-a-jupyter-kernel-for-our-environment",
    "href": "posts/share-hpg-envs/index.html#setup-a-jupyter-kernel-for-our-environment",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Setup a Jupyter Kernel for our environment",
    "text": "Setup a Jupyter Kernel for our environment\nOften, we want to use the environment in a Jupyter notebook. To do that, we can create our own Jupyter Kernel.\n\nAdd the jupyterlab package\nIn order to use an environment in Jupyter, we need to make sure we install the jupyterlab package in the environment:\nmamba install jupyterlab\n\n\nCopy the template_kernel folder to your path\nOn HiPerGator, Jupyter looks in two places for kernels when you launch a notebook:\n\n/apps/jupyterhub/kernels/ for the globally available kernels that all users can use. (Also a good place to look for troubleshooting getting your own kernel going)\n~/.local/share/jupyter/kernels for each user. (Again, your home directory and the .local folder is hidden since it starts with a dot)\n\nMake the ~/.local/share/jupyter/kernels directory: mkdir -p ~/.local/share/jupyter/kernels\nCopy the /apps/jupyterhub/template_kernel folder into your ~/.local/share/jupyter/kernels directory:\ncp -r /apps/jupyterhub/template_kernel/ ~/.local/share/jupyter/kernels/hfrl\n\n\n\n\n\n\nNote\n\n\n\nThis also renames the folder in the copy. It is important that the directory names be distinct in both your directory and the global /apps/jupyterhub/kernels/ directory.\n\n\n\n\nEdit the template_kernel files\nThe template_kernel directory has four files: the run.sh and kernel.json files will need to be edited in a text editor. We will use nano in this tutorial. The logo-64X64.png and logo-32X32.png are icons for your kernel to help visually distinguish it from others. You can upload icons of those dimensions to replace the files, but they need to be named with those names.\n\nEdit the kernel.json file\nLet‚Äôs start editing the kernel.json file. As an example, we can use:\nnano ~/.local/share/jupyter/kernels/hfrl/kernel.json\nThe template has most of the information and notes on what needs to be updated. Edit the file to look like:\n{\n \"language\": \"python\",\n \"display_name\": \"MNE v1.x\",\n \"argv\": [\n  \"~/.local/share/jupyter/kernels/mne_1_x/run.sh\",\n  \"-f\",\n  \"{connection_file}\"\n ]\n}\n\n\nEdit the run.sh file\nThe run.sh file needs the path to the python application that is in our environment. The easiest way to get that is to make sure the environment is activated and run the command: which python\nThe path it outputs should look something like: /blue/group/share/conda/envs/mne_1_x/bin/python\nCopy that path.\nEdit the run.sh file with nano:\nnano ~/.local/share/jupyter/kernels/mne_1_x/run.sh\nThe file should looks like this, but with your path:\n#!/usr/bin/bash\n\nexec /blue/akeil/share/conda/envs/mne_1_x/bin/python -m ipykernel \"$@\"\nIf you are doing this in a Jupyter session, refresh your page. If not, launch Jupyter.\nYour kernel should be there ready for you to use!"
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#working-with-yml-files",
    "href": "posts/share-hpg-envs/index.html#working-with-yml-files",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Working with yml files",
    "text": "Working with yml files\n\nExport your environment to an environment.yml file\nNow that you have your environment working, you may want to document its contents and/or share it with others. The environment.yml file defines the environment and can be used to build a new environment with the same setup.\nTo export an environment file from an existing environment, run:\nconda env export &gt; mne_1_x.yml\nYou can inspect the contents of this file with cat mne_1_x.yml. This file defines the packages and versions that make up the environment as it is at this point in time. Note that it also includes packages that were installed via pip.\n\n\nCreate an environment from a yaml file\nIf you share the environment yaml file created above with another user, they can create a copy of your environment using the command:\nconda env create ‚Äìfile mne_1_x.yml\nThey may need to edit the last line to change the location to match where they want their environment created."
  },
  {
    "objectID": "posts/share-hpg-envs/index.html#group-environments",
    "href": "posts/share-hpg-envs/index.html#group-environments",
    "title": "Using Custom Python environments for Jupyter Notebooks in HiPerGator",
    "section": "Group environments",
    "text": "Group environments\nIt is possible to create a shared environment accessed by a group on HiPerGator, storing the environment in, for example, /blue/akeil/share/conda. In general, this works best if only one user has write access to the environment. All installs should be made by that one user and should be communicated with the other users in the group."
  },
  {
    "objectID": "posts/repo_best_practices/index.html",
    "href": "posts/repo_best_practices/index.html",
    "title": "Open Science: Best Practices for Repos",
    "section": "",
    "text": "Q: How does someone use a repo to accelerate onboarding, improve accessibility and integration, and overall reduce technical burden?\nA: Using a repository (repo) can be an effective way to accelerate onboarding, improve accessibility and integration, and reduce technical burden. Here are some steps to achieve these goals:\n\nClear documentation:\n\nProvide a well-structured README.md that serves as a guide to the repo. It should include an overview of the project, its purpose, key features, and how to set it up.\n\nInclude a comprehensive wiki or documentation folder with detailed instructions on installation, usage, APIs, code examples, and troubleshooting tips.\n\nStructured and organized code:\n\nOrganize the codebase into logical and modular components, with clear separation of concerns.\nFollow consistent coding conventions and naming conventions.\n\nInclude comments throughout the code to explain the purpose and logic of each section.\n\nVersion control and branching strategy:\n\nUse a version control system like Git to maintain a clean and organized commit history.\nImplement a branching strategy that makes it easy for new team members to understand the workflow, submit changes, and merge code.\n\nAutomated testing and continuous integration:\n\nSet up automated tests to ensure code quality and prevent regressions.\nUse a continuous integration (CI) system to automatically build and test code changes.\n\nAccessible dependencies and resources:\n\nClearly list and document all dependencies and required resources, like libraries, frameworks, or APIs.\n\nMake it easy to manage and install dependencies using package managers like npm, pip, or Maven.\n\nIssue tracking and collaboration:\n\nUse an issue tracking system to manage bugs, enhancements, and other tasks.\n\nEncourage collaboration through pull requests, code reviews, and discussions on issues.\n\nTraining and support:\n\nOffer training materials, like videos or tutorials, to help new team members get up to speed.\nProvide a support channel, like a dedicated Slack or Discord group, where new members can ask questions and get assistance.\n\nAPI and data integration:\n\nDesign well-structured and consistent APIs for easy integration with other systems.\nDocument API usage, including authentication, endpoints, and example requests/responses.\nEnsure data formats are consistent and well-documented for seamless integration with other tools and systems.\n\n\nSummary: By following these steps, you can create a repo that accelerates onboarding, improves accessibility and integration, and overall reduces technical burden for your team."
  },
  {
    "objectID": "posts/dlc-gui-hpg-desktop/index.html#log-on-to-hipergator-hpg-openondemand",
    "href": "posts/dlc-gui-hpg-desktop/index.html#log-on-to-hipergator-hpg-openondemand",
    "title": "DeepLabCut GUI on HiPerGator Desktop",
    "section": "Log on to HiPerGator (HPG) OpenOnDemand",
    "text": "Log on to HiPerGator (HPG) OpenOnDemand\nUse a Chrome Browser for best results.\nImportantly, go to (linked UFRC page) for details & video on HiPerGator-OpenOnDemand\n\n\n\n\n\n\n\nFigure¬†1: Type then select your institution.\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Use 2-factor authentication just like Canvas or Gatormail.\n\nType this address in the navigation bar: - https://ondemand.rc.ufl.edu\nYou will need to type then select University of Florida for your home organization."
  },
  {
    "objectID": "posts/dlc-gui-hpg-desktop/index.html#use-hpg-openondemand-to-start-the-interactive-app-hipergator-desktop",
    "href": "posts/dlc-gui-hpg-desktop/index.html#use-hpg-openondemand-to-start-the-interactive-app-hipergator-desktop",
    "title": "DeepLabCut GUI on HiPerGator Desktop",
    "section": "Use HPG OpenOnDemand to start the Interactive App: HiPerGator Desktop",
    "text": "Use HPG OpenOnDemand to start the Interactive App: HiPerGator Desktop\nWhen options for the desktop are available (see settings figure below):\n\n10 or more Gb of Max Memory (third text box)\n\n1 to 4 hours for Time Requested (4th text box)\n\nselect Default Cluster Partition (5th box dropdown)\n\nBUT, If you‚Äôre training the model with GPU settings select the GPU cluster partition. [ONLY for neural-network training; Use the Default cluster partition for image labeling and adjusting labels. We have limited GPU resources.]\n\nHowever, with GPU selected you will also need to type gpu:a100:1 in the Generic Resource Request field (2nd text box from bottom of page.)\n\n\n\nAfter you have typed the settings click blue Launch button(s) as they appear.\n\n\n\n\n\n\nHiPerGator Desktop\n\n\n\n\n\n\n\nFigure¬†3: Start HiPerGator Desktop with appropriate settings (see settings figure), then Launch.\n\n\n\n\nDesktop Settings"
  },
  {
    "objectID": "posts/dlc-gui-hpg-desktop/index.html#open-terminal-to-enter-3-lines-of-command-line-code-to-start-dlc-gui",
    "href": "posts/dlc-gui-hpg-desktop/index.html#open-terminal-to-enter-3-lines-of-command-line-code-to-start-dlc-gui",
    "title": "DeepLabCut GUI on HiPerGator Desktop",
    "section": "Open Terminal to enter 3 lines of command-line code to start DLC GUI",
    "text": "Open Terminal to enter 3 lines of command-line code to start DLC GUI\n\nClick on Applications\n\nIn HiPerGator Linux Desktop, the Applications button is on the top-left corner.\n\nClick on Terminal Emulator\n\n In the Terminal (i.e., black, command line interface) type the following 3 lines:\n\ncd blue_psy4911\nmodule load deeplabcut\npython -m deeplabcut"
  },
  {
    "objectID": "posts/dlc-gui-hpg-desktop/index.html#point-dlc-to-a-config.yaml-file-in-existing-project",
    "href": "posts/dlc-gui-hpg-desktop/index.html#point-dlc-to-a-config.yaml-file-in-existing-project",
    "title": "DeepLabCut GUI on HiPerGator Desktop",
    "section": "Point DLC to a config.yaml file in existing project",
    "text": "Point DLC to a config.yaml file in existing project\n\nClick on tab, Manage Project, when the DeepLabCut GUI appears.\nThen, click Load existing project & Browse in order to find config.yaml settings file.\nThe folder location is something like:\n\n/blue/psy4911//try-dlc/DeepLabCut/examples/openfield-Pranav-2018-10-30/config.yaml\n\n\nOnce you select config.yaml click the open button and the click ok again."
  },
  {
    "objectID": "posts/dlc-gui-hpg-desktop/index.html#point-dlc-to-a-labeled-videos-directory-and-load-the-images",
    "href": "posts/dlc-gui-hpg-desktop/index.html#point-dlc-to-a-labeled-videos-directory-and-load-the-images",
    "title": "DeepLabCut GUI on HiPerGator Desktop",
    "section": "Point DLC to a labeled videos directory and load the ‚Äúimages‚Äù",
    "text": "Point DLC to a labeled videos directory and load the ‚Äúimages‚Äù"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UF-Open-Science-Wiki",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nConvolution and deconvolution: an Illustration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding How MNE-BIDS_pipeline builds\n\n\n\n\n\n\nbot\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSSH to HPG3 and run MNE-BIDS-Pipeline\n\n\n\n\n\n\nRPM\n\n\nAug 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Solving Code in 20 Questions\n\n\n\n\n\n\nRPM\n\n\nApr 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPost template\n\n\n\n\n\n\nRPM\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Science: Best Practices for Repos\n\n\n\n\n\n\nbot\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPosting Wiki Content\n\n\n\nPython\n\n\ncode\n\n\nAI\n\n\ntutorial\n\n\n\n\n\n\n\nIsabella Fleites\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMy Wiki Document\n\n\nMy trial wiki doc\n\n\n\nAlison Ryan\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMNE Abstract\n\n\n\n\n\n\nIsabella Fleites\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a wiki document.\n\n\n\n\n\n\nRPM\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSharing Customized Python Environments\n\n\n\nPython\n\n\ncode\n\n\nAI\n\n\ntutorial\n\n\n\n\n\n\n\nRPM\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeepLabCut GUI on HiPerGator Desktop\n\n\n\nPython\n\n\ncode\n\n\nAI\n\n\ntutorial\n\n\n\nSteps to start DeepLabCut in graphical user interface mode: 1. Log on to HiPerGator (HPG)  2. Use HPG OpenOnDemand to start the Interactive App: *HiPerGator‚Ä¶\n\n\n\nR. Mears\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSetup HiPerGator for DeepLabCut\n\n\n\nPython\n\n\ncode\n\n\nAI\n\n\ntutorial\n\n\n\nTips to setup your account so that you can use DeepLabCut\n\n\n\nR Mears\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nR-tour in 6 pages\n\n\n\nR\n\n\ncode\n\n\ndata wrangling\n\n\nslides\n\n\ntutorial\n\n\n\nIntroduction to Using R and Tidyverse\n\n\n\nR Mears\n\n\nJul 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GitHub for Open Science\n\n\n\ncode\n\n\ndevelopment\n\n\norganization\n\n\nversion control\n\n\nslides\n\n\ntutorial\n\n\n\nIntroduction to Version control for doing open science\n\n\n\nR Mears\n\n\nJul 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMNE-Python: EEG\n\n\n\nPython\n\n\ncode\n\n\npipeline\n\n\nworkflow\n\n\nMNE\n\n\nEEG\n\n\nslides\n\n\ntutorial\n\n\n\nIntroduction to Analysis Pipelines for EEG\n\n\n\nR Mears\n\n\nJun 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Code?\n\n\n\nPython\n\n\ncode\n\n\npipeline\n\n\nworkflow\n\n\nMNE\n\n\nEEG\n\n\nslides\n\n\n\nRationale for scripting with MNE-Python\n\n\n\nRyan Mears\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSEA MatLab tutorials\n\n\n\ncode\n\n\nanalysis\n\n\nMatlab\n\n\nEEG\n\n\n\n\n\n\n\nAK\n\n\nJun 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPost template\n\n\n\n\n\n\nRPM\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]