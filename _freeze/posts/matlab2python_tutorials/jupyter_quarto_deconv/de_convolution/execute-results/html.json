{
  "hash": "274359b1b2b4059380a7032b354f3141",
  "result": {
    "markdown": "---\ntitle: 'Convolution and deconvolution: an Illustration'\nsubtitle: This Jupyter notebook in a Quarto slideshow is translated from a Matlab LiveScript. The code visualizes some of the elements of convolution and deconvolution\neditor: visual\n---\n\n# Convolution\n\n## \n\n::: columns\n::: {.column width=\"55%\"}\n-   Convolution: a simple mathematical function that quantifies similarity between a pattern (a \"kernel\") such as the red square wave below with data ( blue rectangularish thingy below).\n:::\n\n::: {.column width=\"45%\"}\nThe convolution (black line) reflects how similar the blue signal is with the kernel at any given time point. Convolution is thus used in all of digital signal processing.\n:::\n:::\n\n![](Convolution_of_spiky_function_with_box2.gif)\n\n## \n\nThe math is simple. We take the kernel, move it to the first portion of the data by lining it up with the first sample point, then we multiply each element of the kernel with the data, sum the results into one new number and write that to that first sample point.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%matplotlib inline\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Convolution in signal generation and in fMRI research. {.smaller}\n\nif we want to generate a signal where a certain pattern (the kernel) occurs at certain times, we can use convolution to achieve that. Thus, programs liek SPM use convolution of stimulus timinng info with a lernel that resembles the typical BOLD (fMRI) response as the kernel to generate fake, ideal, fMRI activation for comparison/correlation with the real data:\n\n::: {.cell output-location='column-fragment' execution_count=2}\n``` {.python .cell-code}\nonsets = np.zeros(120)\nonsets[1:11:1] = 1\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n## Make a Kernel {.smaller}\n\n-   Now we make a kernel.\\\n-   the inverted gamma function is nice because it looks like fMRI in V1\n-   let's make one that is 10 scans long\n\n::: {.cell output-location='column-fragment' execution_count=3}\n``` {.python .cell-code code-line-numbers=\"1\"}\ng_f = np.arange(0.1,5.1,0.5)\nkernel = [1./math.gamma(i) for i in g_f]\nplt.plot(kernel) #, title('an inverted gamma kernel, length 10')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n## convolve\n\nNow we just convolve the onsets with the kernel and plot the result\n\n::: {.cell output-location='column-fragment' execution_count=4}\n``` {.python .cell-code code-line-numbers=\"1\"}\ng_f = np.arange(0.1,5,0.5)\nkernel = [1./math.gamma(i) for i in g_f]\nplt.plot(kernel) #, title('an inverted gamma kernel, length 10')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## longer onsets\n\nSo this basically puts each 10 second kernel at the location where the onset is. There were 11 seconds between onsets, so this is like copy paste, but how about when the kernel is longer than the interval between onsets?\n\n::: {.cell output-location='column-fragment' execution_count=5}\n``` {.python .cell-code code-line-numbers=\"1\"}\nconvolution  = np.convolve(onsets, kernel); \n\nplt.plot(convolution) #, title('onset vector convolved with canonical BOLD response')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n## but\n\nSo this basically puts each 10 second kernel at the location where the onset is. There were 11 seconds between onsets, so this is like copy paste, but how about when the kernel is longer than the interval between onsets?\n\n::: {.cell output-location='column-fragment' execution_count=6}\n``` {.python .cell-code code-line-numbers=\"1\"}\ng_f=np.arange(0.1,5.1,0.25)\nkernel = [1./math.gamma(i) for i in g_f] # this Kernel is twice as long\nconvolution  = np.convolve(onsets, kernel); \n#figure\nplt.plot(convolution) #, title('Convolution with temporal summation')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n## overlaps\n\nWhen the convolution encounters overlap, then temporal summation results, because convolution is a process of shifting the kernel, multiplying element-wise, and summation to one new value, rinse and repeat. Because of the shifting and summing up portion of the algorithm, if the Kernel spans multiple, similar events, it will show temporal summation.\n\n## even better...\n\nThis is more interesting with variable inter-onset times.\n\n::: {.cell output-location='column-fragment' execution_count=7}\n``` {.python .cell-code code-line-numbers=\"1\"}\nonsets = np.zeros((120,1))\n\nonsets[[3,14,22,36,46,50,66,86,91,106,115],] = 1; #simple case where a stimulus is on every 11 scans\nprint(onsets.shape)\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(120, 1)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\n## Now we convolve these onset times with the same kernel\n\n::: {.cell output-location='column-fragment' execution_count=8}\n``` {.python .cell-code code-line-numbers=\"1\"}\nconvolution  = np.convolve(onsets.squeeze(), kernel); \nplt.plot(convolution) #, title('Convolution of random ITIs')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n## what happened?\n\nLonger intervals between onsets prompt complete reduction to baseline, temporal proximity prompts smeared, overlapping events. How about gamma-shaped responses to stimuli that are longer than one image?\n\n::: {.cell output-location='column-fragment' execution_count=9}\n``` {.python .cell-code code-line-numbers=\"2,3:5\"}\nonsets = np.zeros((120,1))\nset0=[3,14,22,36,46,50,66,86,91,106,115];\nset1=[i+1 for i in set0]\nset2=[i+2 for i in set0]\nset3=[i+3 for i in set0]\nonsets[set0,] = 1; #simple case where a stimulus is on every 11 scans\nonsets[set1,] = 1;\n\nonsets[set2,] = 1;#simple case where a stimulus is on every 11 scans\nonsets[set3,] = 1;\n##onsets[[3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]+2,] = 1;\n#onsets[[3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]+3,] = 1;\nplt.plot(onsets) #, title ('simulated stimulus onsets, every 11 scans')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n## next\n\nNow we convolve these onset times with the same kernel\n\n::: {.cell output-location='column-fragment' execution_count=10}\n``` {.python .cell-code code-line-numbers=\"1\"}\nconvolution  = np.convolve(onsets.squeeze(), kernel); \n\nplt.plot(convolution) #, title('Convolution with 4-TR-long events'), ylabel('note the scale')\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n# Deconvolution\n\nAnd now something completely different\n\n## Deconvolution\n\ndeconvolution is the process where we wish to estimate the Kernel from known data and known event-times. This is a version of the so-called inverse problem , and we solve it with a regression. Let's start with the simulated we we have:\n\n::: {.cell output-location='column-fragment' execution_count=11}\n``` {.python .cell-code code-line-numbers=\"|4|7\"}\nconvolution = np.random.rand(200) # Replace with your own convolution array\nonsets = np.random.rand(120) # Replace with your own onsets array\n\nX = np.zeros((len(convolution[0:120]), 20))\ntemp = onsets.copy()\n\nfor i in range(20):\n    X[:, i] = temp\n    temp = np.concatenate(([0], temp[:-1]))\n\nplt.pcolor(X, cmap='gray')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n## \n\n::: {.cell output-location='column-fragment' execution_count=12}\n``` {.python .cell-code code-line-numbers=\"|1|4,7\"}\nPX = np.linalg.pinv(X)\n\nplt.pcolor(PX, cmap='gray')\nplt.show()\n\nh = PX @ convolution[0:120].T\n```\n\n::: {.cell-output .cell-output-display}\n![](de_convolution_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\n## \n\n::: {.cell output-location='column-fragment' execution_count=13}\n``` {.python .cell-code code-line-numbers=\"|1,12|13,14|16\"}\nconvolution = np.random.rand(200) # Replace with your own convolution array\nonsets = [3, 14, 22, 36, 46, 50, 66, 86, 91, 106, 115]\n\n### WHAT IS hrf_est()? ###\n#h = hrf_est(convolution[0:120], onsets, 20)\n\n# COULD IT BE?\n\n# def hrf_est(convolution, onsets, n):\n#     X = np.zeros(((convolution).,n))\n#     temp = onsets.copy()\n# \n#     for i in range(n):\n#         X[:,i] = temp\n#         temp = np.concatenate(([0], temp[:-1]))\n# \n#     PX = np.linalg.pinv(X)\n#     h = PX @ convolution[0:120].T\n#     return h\n\n#plt.plot(h)\n#plt.show()\n```\n:::\n\n\n",
    "supporting": [
      "de_convolution_files"
    ],
    "filters": [],
    "includes": {}
  }
}